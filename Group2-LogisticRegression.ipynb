{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba076244",
   "metadata": {},
   "source": [
    "# Classical Statistics Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2ea04",
   "metadata": {},
   "source": [
    "Let's start with loading our data, splitting it, then training a model on all possible input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562ca831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70309c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"bankruptcy.csv\")\n",
    "df1['Intercept'] = 1  # allows the regression to have an intercept\n",
    "x_train, x_test = train_test_split(df1[['X1', 'X2', 'X3', 'X4', 'Intercept']], \n",
    "                                   test_size=0.2, random_state=120)\n",
    "y_train, y_test = train_test_split(df1['Group'], test_size=0.2, random_state=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e89e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.317232\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Group   No. Observations:                   36\n",
      "Model:                          Logit   Df Residuals:                       31\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Tue, 26 Mar 2024   Pseudo R-squ.:                  0.5382\n",
      "Time:                        19:23:38   Log-Likelihood:                -11.420\n",
      "converged:                       True   LL-Null:                       -24.731\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.372e-05\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X1             6.2847      5.729      1.097      0.273      -4.945      17.514\n",
      "X2            -3.6808     12.424     -0.296      0.767     -28.032      20.670\n",
      "X3             2.9764      1.177      2.529      0.011       0.670       5.283\n",
      "X4            -2.2975      3.784     -0.607      0.544      -9.714       5.119\n",
      "Intercept     -4.5690      2.522     -1.812      0.070      -9.511       0.373\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "full_model = sm.Logit(y_train, x_train)\n",
    "full_model_fit = full_model.fit()\n",
    "print(full_model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1f843",
   "metadata": {},
   "source": [
    "Now we can see about narrowing the model down. Since the $X_2$ and $X_4$  $p$-values are so high, let's see how the model would fair without them, and compare the Akaike information criterion (AIC) values for the reduced and full models. First, let's just take out $X_1$ for our reduced model, and see how it compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee295ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.318429\n",
      "         Iterations 8\n",
      "\n",
      "   Full model AIC: 32.841\n",
      "Reduced model AIC: 30.927\n"
     ]
    }
   ],
   "source": [
    "# fitting model without X_2\n",
    "x_train2 = x_train[['X1', 'X3', 'X4', 'Intercept']]\n",
    "reduced_model1 = sm.Logit(y_train, x_train2)\n",
    "reduced_model1_fit = reduced_model1.fit()\n",
    "\n",
    "full_model_aic = aic(llf=full_model.loglike(full_model_fit.params), \n",
    "                     nobs=x_train.size, \n",
    "                     df_modelwc=len(full_model_fit.params))\n",
    "print(\"\\n   Full model AIC: {:.3f}\".format(full_model_aic))\n",
    "\n",
    "reduced_model1_aic = aic(llf=reduced_model1.loglike(reduced_model1_fit.params), \n",
    "                        nobs=x_train2.shape[0], \n",
    "                        df_modelwc=len(reduced_model1_fit.params))\n",
    "print(\"Reduced model AIC: {:.3f}\".format(reduced_model1_aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db97205",
   "metadata": {},
   "source": [
    "The AIC of a model measures how much information from the original data is <i>lost</i> by the model, so a lower AIC is better. This means the reduced model is better in comparison to the full model. To see if removing another variable improves it further, let's look at the summary of the reduced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7582259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Group   No. Observations:                   36\n",
      "Model:                          Logit   Df Residuals:                       32\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 26 Mar 2024   Pseudo R-squ.:                  0.5365\n",
      "Time:                        19:33:40   Log-Likelihood:                -11.463\n",
      "converged:                       True   LL-Null:                       -24.731\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.371e-06\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X1             4.8697      2.909      1.674      0.094      -0.831      10.570\n",
      "X3             2.8358      1.051      2.699      0.007       0.776       4.895\n",
      "X4            -2.2554      3.764     -0.599      0.549      -9.633       5.122\n",
      "Intercept     -4.2771      2.258     -1.894      0.058      -8.703       0.149\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(reduced_model1_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34788f",
   "metadata": {},
   "source": [
    "$X_4$ seems to be our next candidate to elimate. So let's try it and see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bdfa0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.323565\n",
      "         Iterations 8\n",
      "\n",
      "1st reduced model AIC: 30.927\n",
      "2nd reduced model AIC: 29.297\n"
     ]
    }
   ],
   "source": [
    "# fitting model without X_2 and X_4\n",
    "x_train3 = x_train[['X1', 'X3', 'Intercept']]\n",
    "reduced_model2 = sm.Logit(y_train, x_train3)\n",
    "reduced_model2_fit = reduced_model2.fit()\n",
    "\n",
    "print(\"\\n1st reduced model AIC: {:.3f}\".format(reduced_model1_aic))\n",
    "\n",
    "reduced_model2_aic = aic(llf=reduced_model2.loglike(reduced_model2_fit.params), \n",
    "                        nobs=x_train3.shape[0], \n",
    "                        df_modelwc=len(reduced_model2_fit.params))\n",
    "print(\"2nd reduced model AIC: {:.3f}\".format(reduced_model2_aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ea747",
   "metadata": {},
   "source": [
    "Even better! Let's see if we have any insignificant variables still:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c6e00ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Group   No. Observations:                   36\n",
      "Model:                          Logit   Df Residuals:                       33\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 26 Mar 2024   Pseudo R-squ.:                  0.5290\n",
      "Time:                        19:35:24   Log-Likelihood:                -11.648\n",
      "converged:                       True   LL-Null:                       -24.731\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.082e-06\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X1             5.5291      2.841      1.946      0.052      -0.039      11.097\n",
      "X3             2.7476      1.025      2.681      0.007       0.739       4.756\n",
      "Intercept     -5.1018      1.945     -2.623      0.009      -8.914      -1.290\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(reduced_model2_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce144d7a",
   "metadata": {},
   "source": [
    "The $X_1$ confidence interval still contains zero, so that means it might not be significant. Let's see how the AIC is when we remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5918a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.393315\n",
      "         Iterations 7\n",
      "\n",
      "2st reduced model AIC: 29.297\n",
      "3nd reduced model AIC: 32.319\n"
     ]
    }
   ],
   "source": [
    "x_train4 = x_train[['X3', 'Intercept']]\n",
    "reduced_model3 = sm.Logit(y_train, x_train4)\n",
    "reduced_model3_fit = reduced_model3.fit()\n",
    "\n",
    "print(\"\\n2st reduced model AIC: {:.3f}\".format(reduced_model2_aic))\n",
    "\n",
    "reduced_model3_aic = aic(llf=reduced_model3.loglike(reduced_model3_fit.params), \n",
    "                        nobs=x_train4.shape[0], \n",
    "                        df_modelwc=len(reduced_model3_fit.params))\n",
    "print(\"3nd reduced model AIC: {:.3f}\".format(reduced_model3_aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccca1c8",
   "metadata": {},
   "source": [
    "So removing it increases the AIC, meaning that the model is better off with $X_1$ after all. This gives us our final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b20ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Group   No. Observations:                   36\n",
      "Model:                          Logit   Df Residuals:                       33\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 26 Mar 2024   Pseudo R-squ.:                  0.5290\n",
      "Time:                        19:42:46   Log-Likelihood:                -11.648\n",
      "converged:                       True   LL-Null:                       -24.731\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.082e-06\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X1             5.5291      2.841      1.946      0.052      -0.039      11.097\n",
      "X3             2.7476      1.025      2.681      0.007       0.739       4.756\n",
      "Intercept     -5.1018      1.945     -2.623      0.009      -8.914      -1.290\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit_model = reduced_model2\n",
    "logit_model_fit = reduced_model2_fit\n",
    "print(logit_model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856fc3e",
   "metadata": {},
   "source": [
    "## Coefficient estimation\n",
    "Since the coefficients are already given in the fitted model, we simply need to exponentiate them to get $\\exp(\\beta_i)$ for $i \\in [0,2]$, giving the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9647a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta        exp(beta_i)\n",
      "X1           251.905737\n",
      "X3            15.605700\n",
      "Intercept      0.006086\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"beta\", \"{:>18}\".format(\"exp(beta_i)\"),end=None)\n",
    "print(logit_model_fit.params.apply(math.exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd2de4",
   "metadata": {},
   "source": [
    "The 95% confidence intervals for these exponentiated coefficients are easily derived as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e82f2f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0           1\n",
      "X1         210.803092  301.022627\n",
      "X3          14.634346   16.641527\n",
      "Intercept    0.005387    0.006875\n"
     ]
    }
   ],
   "source": [
    "ci_df = logit_model_fit.conf_int(0.95)\n",
    "ci_df[0] = ci_df[0].apply(math.exp)\n",
    "ci_df[1] = ci_df[1].apply(math.exp)\n",
    "print(ci_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516778a",
   "metadata": {},
   "source": [
    "These are multiplicative confidence intervals, which means that they are the <i>product</i> of the estimate and $\\exp(\\text{MoE})$ (margin of error) or the estimate and $\\exp(-\\text{MoE})$ for the low end of the interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be75dd",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260718a",
   "metadata": {},
   "source": [
    "## Descision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2188d",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94e7ca",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e19516",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
