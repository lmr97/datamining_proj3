{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba076244",
   "metadata": {},
   "source": [
    "# Classical Statistics Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2ea04",
   "metadata": {},
   "source": [
    "Let's start with loading our data, splitting it, then training a model on all possible input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "562ca831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70309c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"bankruptcy.csv\")\n",
    "x_train, x_test = train_test_split(df1.iloc[:,:4], test_size=0.2, random_state=120)\n",
    "y_train, y_test = train_test_split(df1['Group'], test_size=0.2, random_state=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e89e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.380316\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Group   No. Observations:                   36\n",
      "Model:                          Logit   Df Residuals:                       32\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 26 Mar 2024   Pseudo R-squ.:                  0.4464\n",
      "Time:                        15:16:58   Log-Likelihood:                -13.691\n",
      "converged:                       True   LL-Null:                       -24.731\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.282e-05\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "X1             1.2130      4.293      0.283      0.778      -7.201       9.627\n",
      "X2             7.2314      9.949      0.727      0.467     -12.268      26.731\n",
      "X3             1.5898      0.709      2.243      0.025       0.201       2.979\n",
      "X4            -6.2519      3.112     -2.009      0.045     -12.351      -0.153\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "full_model = sm.Logit(y_train, x_train)\n",
    "full_model_fit = full_model.fit()\n",
    "print(full_model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1f843",
   "metadata": {},
   "source": [
    "Now we can see about narrowing the model down. Since the $X_1$ and $X_2$  $p$-values are so high, let's see how the model would fair without them, and compare the Akaike information criterion (AIC) values for the reduced and full models. First, let's just take out $X_1$ for our reduced model, and see how it compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee295ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.381427\n",
      "         Iterations 7\n",
      "\n",
      "   Full model AIC: 35.383\n",
      "Reduced model AIC: 33.463\n"
     ]
    }
   ],
   "source": [
    "# fitting model without X_1\n",
    "x_train2 = x_train.iloc[:,1:]\n",
    "reduced_model1 = sm.Logit(y_train, x_train2)\n",
    "reduced_model1_fit = reduced_model1.fit()\n",
    "\n",
    "full_model_aic = aic(llf=full_model.loglike(full_model_fit.params), \n",
    "                     nobs=x_train.size, \n",
    "                     df_modelwc=len(full_model_fit.params))\n",
    "print(\"\\n   Full model AIC: {:.3f}\".format(full_model_aic))\n",
    "\n",
    "reduced_model1_aic = aic(llf=reduced_model1.loglike(reduced_model1_fit.params), \n",
    "                        nobs=x_train2.shape[0], \n",
    "                        df_modelwc=len(reduced_model1_fit.params))\n",
    "print(\"Reduced model AIC: {:.3f}\".format(reduced_model1_aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db97205",
   "metadata": {},
   "source": [
    "The AIC of a model measures how much information from the original data is <i>lost</i> by the model, so a higher AIC is worse. This means the reduced model is (slightly) worse in comparison to the full model. We can see that removing $X_2$ produces an even higher AIC below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9bdfa0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.452422\n",
      "         Iterations 7\n",
      "\n",
      "   Full model AIC: 35.383\n",
      "Reduced model AIC: 36.574\n"
     ]
    }
   ],
   "source": [
    "# fitting model without X_1 and X_2\n",
    "x_train3 = x_train.iloc[:,2:]\n",
    "reduced_model2 = sm.Logit(y_train, x_train3)\n",
    "reduced_model2_fit = reduced_model2.fit()\n",
    "\n",
    "full_model_aic = aic(llf=full_model.loglike(full_model_fit.params), \n",
    "                     nobs=x_train.size, \n",
    "                     df_modelwc=len(full_model_fit.params))\n",
    "print(\"\\n   Full model AIC: {:.3f}\".format(full_model_aic))\n",
    "\n",
    "reduced_model2_aic = aic(llf=reduced_model2.loglike(reduced_model2_fit.params), \n",
    "                        nobs=x_train3.shape[0], \n",
    "                        df_modelwc=len(reduced_model2_fit.params))\n",
    "print(\"Reduced model AIC: {:.3f}\".format(reduced_model2_aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ea747",
   "metadata": {},
   "source": [
    "Since the rest of the variables' coefficients are significant, we'll probably only get higher AICs by removing them, so we should proceed with the full model as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c6e00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856fc3e",
   "metadata": {},
   "source": [
    "### Coefficient estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be75dd",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260718a",
   "metadata": {},
   "source": [
    "### Descision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2188d",
   "metadata": {},
   "source": [
    "# Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94e7ca",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e19516",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
